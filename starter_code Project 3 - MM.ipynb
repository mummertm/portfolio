{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Web Scraping for Indeed.com & Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34681254-c802-462f-829d-8894d0772d08"
   },
   "source": [
    "In this project, we will practice two major skills: collecting data by scraping a website and then building a binary classifier.\n",
    "\n",
    "We are going to collect salary information on data science jobs in a variety of markets. Then using the location, title, and summary of the job we will attempt to predict the salary of the job. For job posting sites, this would be extraordinarily useful. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries from other listings can help guide negotiations.\n",
    "\n",
    "Normally, we could use regression for this task; however, we will convert this problem into classification and use a random forest classifier, as well as another classifier of your choice; either logistic regression, SVM, or KNN. \n",
    "\n",
    "- **Question**: Why would we want this to be a classification problem?\n",
    "- **Answer**: While more precision may be better, there is a fair amount of natural variance in job salaries - predicting a range be may be useful.\n",
    "\n",
    "Therefore, the first part of the assignment will be focused on scraping Indeed.com. In the second, we'll focus on using listings with salary information to build a model and predict additional salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "# Step 1: Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Importing some libraries to use for scraping\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'URL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-94d489930d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msoup1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'URL' is not defined"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "r = urllib.urlopen(URL).read()\n",
    "soup1 = BeautifulSoup(r, 'html.parser', from_encoding=\"utf-8\")\n",
    "print type(soup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup1.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one result more closely. A single result looks like\n",
    "```JSON\n",
    "<div class=\" row result\" data-jk=\"2480d203f7e97210\" data-tn-component=\"organicJob\" id=\"p_2480d203f7e97210\" itemscope=\"\" itemtype=\"http://schema.org/JobPosting\">\n",
    "<h2 class=\"jobtitle\" id=\"jl_2480d203f7e97210\">\n",
    "<a class=\"turnstileLink\" data-tn-element=\"jobTitle\" onmousedown=\"return rclk(this,jobmap[0],1);\" rel=\"nofollow\" target=\"_blank\" title=\"AVP/Quantitative Analyst\">AVP/Quantitative Analyst</a>\n",
    "</h2>\n",
    "<span class=\"company\" itemprop=\"hiringOrganization\" itemtype=\"http://schema.org/Organization\">\n",
    "<span itemprop=\"name\">\n",
    "<a href=\"/cmp/Alliancebernstein?from=SERP&campaignid=serp-linkcompanyname&fromjk=2480d203f7e97210&jcid=b374f2a780e04789\" target=\"_blank\">\n",
    "    AllianceBernstein</a></span>\n",
    "</span>\n",
    "<tr>\n",
    "<td class=\"snip\">\n",
    "<nobr>$117,500 - $127,500 a year</nobr>\n",
    "<div>\n",
    "<span class=\"summary\" itemprop=\"description\">\n",
    "C onduct quantitative and statistical research as well as portfolio management for various investment portfolios. Collaborate with Quantitative Analysts and</span>\n",
    "</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this has some more verbose elements removed, we can see that there is some structure to the above:\n",
    "- The salary is available in a nobr element inside of a td element with class='snip.\n",
    "- The title of a job is in a link with class set to jobtitle and a data-tn-element=\"jobTitle.\n",
    "- The location is set in a span with class='location'.\n",
    "- The company is set in a span with class='company'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write 4 functions to extract each item: location, company, job, and salary.Â¶\n",
    "Example\n",
    "```python\n",
    "def extract_location_from_result(result):\n",
    "    return result.find ...\n",
    "```\n",
    "\n",
    "##### - Make sure these functions are robust and can handle cases where the data/field may not be available.\n",
    ">- Remember to check if a field is empty or None for attempting to call methods on it\n",
    ">- Remember to use try/except if you anticipate errors.\n",
    "\n",
    "- **Test** the functions on the results above and simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_location(row_result):\n",
    "    ### Finding the item at span where the class equals location and returning the results\n",
    "    x = row_result.find('span', {'class':'location'}).renderContents()\n",
    "    if 'itemprop=\"addressLocality' in x:\n",
    "        x =  i.find('span', {'itemprop' : 'addressLocality'}).renderContents()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_company(row_result):\n",
    "    ## try to find the company\n",
    "    try:\n",
    "        #### FInding the company entry\n",
    "        company = row_result.find('span', {'class':'company'}).text\n",
    "        ### The text results have a lot of '\\n' in, so I replaced those\n",
    "        company = company.replace('\\n', '')\n",
    "        ### cleaning the extra spaces out of company\n",
    "        while company[0]==' ':\n",
    "            company= company[1:]\n",
    "        return company\n",
    "    ### if you get an error -ie, the company doesn't exist, return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_job(row_result):\n",
    "    return row_result.find('a', attrs={'class':'turnstileLink'}).attrs['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_salary(row_result):\n",
    "    try:\n",
    "        ### The try and except are very important - you need to \n",
    "        return row_result.find('td', {'class' : 'snip'}).find('nobr').text\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_desc(row_result):\n",
    "    try:\n",
    "        return row_result.find('span', {'class':'summary'}).renderContents\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results. We can do this by examining the URL above.\n",
    "- \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "There are two query parameters here we can alter to collect more results, the l=New+York and the start=10. The first controls the location of the results (so we can try a different city). The second controls where in the results to start and gives 10 results (thus, we can keep incrementing by 10 to go further in the list).\n",
    "##### Complete the following code to collect results from multiple cities and starting points.\n",
    "- Enter your city below to add it to the search\n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YOUR_CITY = 'Washington'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_template = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "max_results_per_city = 3000 # Set this to a high-value (5000) to generate more results. \n",
    "# Crawling more results will also take much longer. First test your code on a small number of results and then expand.\n",
    "\n",
    "results = []\n",
    "frame= []\n",
    "\n",
    "for city in set(['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "    'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', YOUR_CITY]):\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        URL =  \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\".format(city, start)\n",
    "        r = urllib.urlopen(URL).read()\n",
    "        soup = BeautifulSoup(r, 'html.parser', from_encoding=\"utf-8\")\n",
    "        results.append(soup)\n",
    "        pass\n",
    "### Appends all of the soup objects to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now that I have a list of the soup objects I will pass through each object\n",
    "for i in results:\n",
    "    ### for each object, pass through each results table and extract the location\n",
    "    for result in i.find_all('div', class_=['row', 'result']):\n",
    "        location = extract_location(result)\n",
    "        company = extract_company(result)\n",
    "        job = extract_job(result)\n",
    "        salary = extract_salary(result)\n",
    "        desc = extract_desc(result)\n",
    "        ### assign the extracted restuls to a dictionary and then add that dictionary to a list\n",
    "        frame.append({'location':location, 'company':company, 'job':job, 'salary':salary, 'desc':desc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "6e259594-1c52-436b-ab9e-527e071941c1"
   },
   "outputs": [],
   "source": [
    "### create a new dataframe with columns that match the dictionary values\n",
    "df = pd.DataFrame(columns=['location', 'company', 'job','salary', 'desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### add the extracted results to a new datadrame\n",
    "\n",
    "df = df.append(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "Lastly, we need to clean up salary data. \n",
    "\n",
    "1. Only a small number of the scraped results have salary information - only these will be used for modeling.\n",
    "1. Some of the salaries are not yearly but hourly or weekly, these will not be useful to us for now\n",
    "1. Some of the entries may be duplicated\n",
    "1. The salaries are given as text and usually with ranges.\n",
    "\n",
    "#### Find the entries with annual salary entries, by filtering the entries without salaries or salaries that are not yearly (filter those that refer to hour or week). Also, remove duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df.shape\n",
    "\n",
    "## remove duplicates\n",
    "\n",
    "df.drop_duplicates()\n",
    "\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9984c0875e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Save my dataframe to a CSV so I can keep the results each time I do a search.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/desktop/CSV/Indeed_Data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "### Save my dataframe to a CSV so I can keep the results each time I do a search.\n",
    "df.to_csv('~/desktop/CSV/Indeed_Data.csv', mode='a', header =False, encoding= 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "\n",
    "- Read CSV \n",
    "- Remove duplicates\n",
    "- Drop missing values\n",
    "- Convert salary into floats\n",
    "- Simplify location\n",
    "    - Keep only city names\n",
    "    - Map those city names to a region\n",
    "- Create my target column: above or below the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the saved CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196815, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading back in the CSV files where I've been saving my results\n",
    "\n",
    "new_df = pd.read_csv('~/desktop/CSV/Indeed_Data.csv', names=[ 'location','company',  'title','salary', 'desciprtion'])\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196815, 5)\n",
      "(58684, 5)\n"
     ]
    }
   ],
   "source": [
    "## Removing duplicates\n",
    "print new_df.shape\n",
    "## remove duplicates\n",
    "new_df.drop_duplicates(inplace= True)\n",
    "print new_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dropping rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58684, 5)\n",
      "(3424, 5)\n"
     ]
    }
   ],
   "source": [
    "## drop rows with any missing values - including salary data:\n",
    "print new_df.shape\n",
    "indeed_data = new_df.dropna()\n",
    "print indeed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I'm left with over 3,000 results - that looks like plenty to do some analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Salary Column into Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5       $80,000 - $120,000 a year\n",
      "8        $50,000 - $80,000 a year\n",
      "22                   $120 an hour\n",
      "26       $57,200 - $85,800 a year\n",
      "30                 $43,794 a year\n",
      "32                 $46,831 a year\n",
      "57                 $65,000 a year\n",
      "62       $40,800 - $79,100 a year\n",
      "66                    $75 an hour\n",
      "88       $66,400 - $99,600 a year\n",
      "116                $60,000 a year\n",
      "179               $150,000 a year\n",
      "212      $39,983 - $55,500 a year\n",
      "216    $100,000 - $130,000 a year\n",
      "218      $63,696 - $94,557 a year\n",
      "236             $10 - $15 an hour\n",
      "239      $45,000 - $55,000 a year\n",
      "240      $45,000 - $77,000 a year\n",
      "245             $10 - $15 an hour\n",
      "248      $50,000 - $55,000 a year\n",
      "Name: salary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Cleaning up the salary column\n",
    "print indeed_data['salary'][0:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I see three types of data here: \n",
    " 1. Yearly salary - I need to convert those to integer\n",
    " 2. Salary range -  I need to convert this to integers and then average them\n",
    " 3. Monthly Salary - I will clean and multiple by 12 to convert to yearly.\n",
    " 3. Hourly salary - I should drop these.  I don't have enough information about the job to know whether I can extrapolate this to a yearly rate.  It might not be a full-time posting.\n",
    " 4. Hourly range - I need to average these and then extrapolate to a yearly rate\n",
    " 5. Weekly salary/range\n",
    " 6. Month salary/range\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## First, I created a function that took a string and returned the average of the numbers contained in the string\n",
    "def extract_int(sal_col_value):\n",
    "    l=[]\n",
    "    for t in sal_col_value.split(): # for each word in the string, word being an object separated by spaces\n",
    "        t = t.replace('$', '') # If that word has a '$' or a ',' - remove it\n",
    "        t = t.replace(',' , '')\n",
    "        try:\n",
    "            l.append(float(t))  # Attempt to append the float of that word to a list\n",
    "        except ValueError: # if it doesn't work - ie, the 'word' is not a number, skip that word\n",
    "            pass\n",
    "    if len(l) >= 2:  ## If there are two numbers in the list it was a range so average those\n",
    "        return np.mean(l)\n",
    "    elif len(l) <= 1: ##If there's only one, just return that\n",
    "        return l[0]\n",
    "    \n",
    "    \n",
    "# The second function determines if a salary is monthly, hourly, weekly or daily and applies the appropriate multiplier\n",
    "# to convert it into yearly.\n",
    "\n",
    "def normal_func(x):\n",
    "    if 'month' in x.split():\n",
    "        k = ((extract_int(x)) * 12)\n",
    "    elif 'hour' in x.split():\n",
    "        k = ((extract_int(x)) * 2080)\n",
    "    elif 'week' in x.split():\n",
    "        k = ((extract_int(x)) * 52)\n",
    "    elif 'day' in x.split():\n",
    "        k = ((extract_int(x)) * 260)\n",
    "    else:\n",
    "        k =  extract_int(x)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "## Apply the function to the salary data and add the result to a new column\n",
    "\n",
    "indeed_data['salary_transformed'] = indeed_data['salary'].apply(normal_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>salary</th>\n",
       "      <th>desciprtion</th>\n",
       "      <th>salary_transformed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$80,000 - $120,000 a year</td>\n",
       "      <td>\\rWorks with multi-functional teams focused on...</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>$50,000 - $80,000 a year</td>\n",
       "      <td>\\rHands-on experience with NGS data analysis. ...</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      location             company                              title  \\\n",
       "5  Houston, TX  Platinum Solutions                     Data Scientist   \n",
       "8  Houston, TX            Genialis  Bioinformatics Software Developer   \n",
       "\n",
       "                      salary  \\\n",
       "5  $80,000 - $120,000 a year   \n",
       "8   $50,000 - $80,000 a year   \n",
       "\n",
       "                                         desciprtion  salary_transformed  \n",
       "5  \\rWorks with multi-functional teams focused on...            100000.0  \n",
       "8  \\rHands-on experience with NGS data analysis. ...             65000.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The indexes are all out of order - I'll adjust that:\n",
    "\n",
    "indeed_data.reset_index(inplace=True, drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>salary</th>\n",
       "      <th>desciprtion</th>\n",
       "      <th>salary_transformed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$80,000 - $120,000 a year</td>\n",
       "      <td>\\rWorks with multi-functional teams focused on...</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>$50,000 - $80,000 a year</td>\n",
       "      <td>\\rHands-on experience with NGS data analysis. ...</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      location             company                              title  \\\n",
       "0  Houston, TX  Platinum Solutions                     Data Scientist   \n",
       "1  Houston, TX            Genialis  Bioinformatics Software Developer   \n",
       "\n",
       "                      salary  \\\n",
       "0  $80,000 - $120,000 a year   \n",
       "1   $50,000 - $80,000 a year   \n",
       "\n",
       "                                         desciprtion  salary_transformed  \n",
       "0  \\rWorks with multi-functional teams focused on...            100000.0  \n",
       "1  \\rHands-on experience with NGS data analysis. ...             65000.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simplifying Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York, NY                                                                                 191\n",
      "Chicago, IL                                                                                   99\n",
      "Seattle, WA                                                                                   87\n",
      "San Francisco, CA                                                                             85\n",
      "Austin, TX                                                                                    74\n",
      "Los Angeles, CA                                                                               60\n",
      "Washington, DC                                                                                51\n",
      "Seattle, WA 98109 <span style=\"font-size: smaller\">(Westlake area)</span>                     45\n",
      "New York, NY 10017 <span style=\"font-size: smaller\">(Midtown area)</span>                     37\n",
      "New York, NY 10016 <span style=\"font-size: smaller\">(Gramercy area)</span>                    37\n",
      "Manhattan, NY                                                                                 34\n",
      "Philadelphia, PA                                                                              32\n",
      "New York, NY 10018 <span style=\"font-size: smaller\">(Clinton area)</span>                     29\n",
      "New York, NY 10011 <span style=\"font-size: smaller\">(Chelsea area)</span>                     29\n",
      "Chicago, IL 60601 <span style=\"font-size: smaller\">(Loop area)</span>                         28\n",
      "Atlanta, GA                                                                                   27\n",
      "Bellevue, WA                                                                                  27\n",
      "Redmond, WA 98052                                                                             27\n",
      "Chicago, IL 60654 <span style=\"font-size: smaller\">(Loop area)</span>                         27\n",
      "El Segundo, CA 90245                                                                          26\n",
      "Los Angeles, CA 90036                                                                         26\n",
      "South San Francisco, CA 94080                                                                 26\n",
      "New York, NY 10003 <span style=\"font-size: smaller\">(Greenwich Village area)</span>           25\n",
      "Redmond, WA                                                                                   25\n",
      "San Francisco, CA 94103 <span style=\"font-size: smaller\">(South Of Market area)</span>        24\n",
      "South San Francisco, CA                                                                       24\n",
      "Phoenix, AZ                                                                                   24\n",
      "Chicago, IL 60606 <span style=\"font-size: smaller\">(Loop area)</span>                         23\n",
      "McLean, VA                                                                                    23\n",
      "New York, NY 10029 <span style=\"font-size: smaller\">(Yorkville area)</span>                   23\n",
      "                                                                                            ... \n",
      "Wilmington, DE 19801                                                                           1\n",
      "Philadelphia, PA 19107 <span style=\"font-size: smaller\">(City Center East area)</span>         1\n",
      "Pittsburgh, PA 15213 <span style=\"font-size: smaller\">(North Oakland area)</span>              1\n",
      "New York, NY 10119 <span style=\"font-size: smaller\">(Chelsea area)</span>                      1\n",
      "Pittsburgh, PA 15222 <span style=\"font-size: smaller\">(Strip District area)</span>             1\n",
      "Phoenix, AZ 85002 <span style=\"font-size: smaller\">(Central City area)</span>                  1\n",
      "New York, NY 10170 <span style=\"font-size: smaller\">(Murray Hill area)</span>                  1\n",
      "Florham Park, NJ                                                                               1\n",
      "Germantown, MD                                                                                 1\n",
      "Cork, PA                                                                                       1\n",
      "Los Angeles, CA 90031                                                                          1\n",
      "Oakbrook Terrace, IL 60181                                                                     1\n",
      "San Francisco, CA 94109 <span style=\"font-size: smaller\">(Nob Hill area)</span>                1\n",
      "Pasadena, CA 91107                                                                             1\n",
      "Dallas, TX 75247 <span style=\"font-size: smaller\">(Northwest Dallas area)</span>               1\n",
      "Arlington, TX 76019 <span style=\"font-size: smaller\">(Central area)</span>                     1\n",
      "Roswell, GA                                                                                    1\n",
      "Aliquippa, PA 15001                                                                            1\n",
      "Philadelphia, PA 19110 <span style=\"font-size: smaller\">(City Center West area)</span>         1\n",
      "Los Angeles, CA 90001                                                                          1\n",
      "Grand Prairie, TX 75051                                                                        1\n",
      "San Rafael, CA                                                                                 1\n",
      "Duluth, GA 30097                                                                               1\n",
      "Mill Valley, CA 94941                                                                          1\n",
      "Los Angeles, CA 90025                                                                          1\n",
      "Atlanta, GA 30303 <span style=\"font-size: smaller\">(Five Points area)</span>                   1\n",
      "Denver, CO 80219 <span style=\"font-size: smaller\">(Southwestern Denver area)</span>            1\n",
      "Pittsburgh, PA 15201 <span style=\"font-size: smaller\">(Central Lawrenceville area)</span>      1\n",
      "Adelphi, MD                                                                                    1\n",
      "Duluth, GA                                                                                     1\n",
      "Name: location, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print indeed_data.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## There are way too many different location names - I want to simplify these to just the city name\n",
    "\n",
    "def loc_to_city(x):\n",
    "    counter = 0\n",
    "    try:\n",
    "        while x[counter] != ',':\n",
    "            city = x[:counter+1]\n",
    "            counter+= 1\n",
    "    except:\n",
    "        pass\n",
    "    return city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "indeed_data['location'] = indeed_data['location'].apply(loc_to_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a region variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can convert large number of cities to regional values, based on my original city search using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regionalize= {'Atlanta': 'Atlanta', 'Austin': 'Austin' , 'Chicago': 'Chicago', 'Dallas': 'Dallas', \n",
    "    'Denver':'Denver', 'Los Angeles': 'Los Angeles', 'Miami': 'Miami', 'New York': 'New York',\n",
    "    'Oakland': 'San Francisco' , 'Philadelphia': 'Philadelphia', 'Phoenix': 'Phoenix',  'Pittsburgh': 'Pittsburgh', \n",
    "    'Portland': 'Portland', 'San Francisco': 'San Francisco', 'Seattle': 'Seattle', 'Washington': 'Washington', \n",
    "    'Houston': 'Houston', 'South San Francisco': 'San Francisco', 'Manhattan': 'New York', 'Redmond':'Seattle',\n",
    "    'Rockville': 'Washington', 'Bellevue' : 'Seattle', 'Alexandria': 'Washington', 'Arlington' : 'Washington',\n",
    "    'El Segundo' : 'Los Angeles', 'Berkeley': 'San Francisco' , 'McLean': 'Washington', 'Santa Monica': 'Los Angeles', \n",
    "    'Bronx':'New York' , 'Rockville' :'Washington', 'Reston':'Washington', 'Torrance':'Los Angeles',\n",
    "    'San Mateo':'San Francisco','Redwood City': 'San Francisco','King of Prussia':'Philadelphia','Bothell':'Seattle',\n",
    "    'Springfield':'Washington','Silver Spring':'Washington', 'Emeryville': 'Chicago', 'Collegeville':'Philadelphia', \n",
    "    'Jersey City':'New York', 'Pasadena': 'Los Angeles' , 'Bethesda' : 'Washington','Wilmington':'Philadelphia',\n",
    "    'Irving':'Dallas', 'Hillsboro':'Portland', 'West Point':'New York', 'Malvern':'Philadelphia', \n",
    "    'Fort Lauderdale':'Miami' , 'Coral Gables': 'Miami' ,'Aurora' :'Denver' , 'Burbank':'Los Angeles' , \n",
    "    'Boulder':'Denver' , 'Brea':'Los Angeles', 'San Bruno':'San Francisco' , 'Chantilly':'Washington','Alpharetta':'Atlanta',\n",
    "    'Woodland Hills':'Los Angeles', 'Downers Grove':'Chicago', 'Laurel':'Washington' ,'Herndon':'Washington',\n",
    "    'College Park':'Washington','Berkeley Heights':'San Francisco','Duarte':'Los Angeles','Deerfield':'Chicago',\n",
    "    'Plano':'Dallas','San Francisco Bay Area':'San Francisco' , 'Fort Meade' :'Washington','Foster City':'San Francisco',\n",
    "    'Gaithersburg':'Washington','Vancouver':'Portland' ,'Skokie': 'Seattle' ,'Scottsdale':'Phoenix', 'Summit':'New York',\n",
    "    'East Hanover':'New York','Fullerton':'Los Angeles','Abbott Park' :'Chicago' , 'Suitland':'Washington',\n",
    "    'Spring House':'Philadelphia' ,'Valley Stream':'New York','Richardson':'Dallas', 'Greenbelt':'Washington',\n",
    "    'New Hyde Park':'New York' ,'San Ramon':'San Francisco','Itasca':'Chicago','Broomfield':'Denver',\n",
    "    'Marietta':'Philadelphia','Glendale':'Los Angeles' , 'West Hills':'Los Angeles','Beverly Hills':'Los Angeles',\n",
    "    'Northbook':'Chicago' ,'Clark':'Chicago','Bee Cave':'Austin' ,'Cherry Hill':'New York' ,'Franklin Lake':'New York',\n",
    "    'Norcross':'Atlanta','Lemont':'Chicago','Northridge':'Los Angeles','Lisle':'Chicago' ,'Wayne':'Philadelphia',\n",
    "    'Kennesaw':'Atlanta', 'San Carlos':'San Francisco','Beaverton':'Portland','Playa Vista':'Los Angeles',\n",
    "    'Arlington Heights':'Chicago','Terminal Island':'Los Angeles','Plainfield':'Chicago', 'Westminster':'Washington',\n",
    "    'West Chester':'Philadelphia','Fairfax':'Washington' ,'Anaheim':'Los Angeles','Tempe':'Phoenix','Goodyear':'Phoenix',\n",
    "    'Fort Belvoir':'Washington' , 'Germantown':'Washington','Mount Laurel':'Philadelphia','Falls Church':'Washington',\n",
    "    'Horsham':'Philadelphia','Duluth':'Atlanta', 'Van Nuys':'Los Angeles','Bremerton':'Seattle' ,\n",
    "    'Plymouth Meeting':'Philadelphia','Monrovia':'Los Angeles','Golden':'Denver' ,'Kyle':'Austin','Brisbane':'San Francisco',\n",
    "    'Menlo Park':'San Francisco' ,'Everett':'Seattle','Tucker':'Atlanta', 'Venice':'Los Angeles','Brooklyn':'New York',\n",
    "    'Melrose Park':'Chicago','Vienna':'Washington','South Holland':'Chicago','Renton':'Seattle','Paramus':'New York',\n",
    "    'Evanston':'Chicago','Englewood Cliffs':'New York','Novato':'San Francisco','Hollywood':'Los Angeles',\n",
    "    'Lone Tree':'Denver','Glenview':'Chicago','Cerritos':'Los Angeles','Baytown':'Houston','West Hollywood':'Los Angeles',\n",
    "    'Rolling Meadows':'Chicago','Upper Providence':'Philadelphia','Gilbert':'Phoenix','Murray Hill':'New York',\n",
    "    'Spring':'Chicago','Dania Beach': 'Miami', 'Kenilworth':'Chicago','Rosemont':'Chicago','Burlingame':'San Francisco',\n",
    "    'Canoga Park':'Los Angeles' , 'Des Plaines':'Chicago','East Rutherford':'New York','Berwyn':'Philadelphia',\n",
    "    'Parsippany':'New York','Littleton':'Denver', 'Rahway':'New York','Ambler':'Philadelphia' , 'Sausalito':'San Francisco',\n",
    "    'Lewisville':'Dallas' ,'Coppell':'Dallas','West Orange':'New York' ,'Louisville':'Denver','Mount Vernon':'New York',\n",
    "    'Rosslyn':'Washington','Lanham':'Washington','West Mifflin':'Philadelphia','Hoboken':'New York','Miami Lakes':'Miami',\n",
    "    'Culver City':'Los Angeles','Long Beach':'Los Angeles', 'Federal Way':'Seattle','Hayward':'San Francisco',\n",
    "    'Cinnaminson':'Philadelphia','Bronx Zoo':'New York','Allendale':'New York','Fremont':'San Francisco','Chevy Chase':'Washington',\n",
    "    'Moorestown':'Philadelphia','Tysons Corner':'Washington','Universal City':'Los Angeles','Oakbrook Terrace':'Chicago',\n",
    "    'San Pedro':'Los Angeles','Newark':'New York','Whippany':'New York','Redwood Shores':'San Francisco','Kent':'Seattle',\n",
    "    'Los Alamitos':'Los Angeles','Lakewood':'Denver','Florham Park':'New York','Salmon Creek':'Portland','Miramar':'Miami',\n",
    "    'Fayetteville':'Atlanta','Md City':'Washington','Indian Head':'Washington','North Metro':'Atlanta','Leander':'Austin',\n",
    "    'Lombard':'Chicago','Columbia':'Washington','Addison':'Dallas','Woodcliff Lake':'New York','Sugar Land':'Houston',\n",
    "    'Annapolis Junction':'Washington','Roswell':'Atlanta','Upper Gwynedd':'Philadelphia','Conshohocken':'Philadelphia',\n",
    "    'Marina del Rey':'Los Angeles','Mill Valley':'San Francisco','Voorhees':'Philadelphia','Silver Springs':'Washington',\n",
    "    'Highlands Ranch':'Denver','Madison':'New York','Blue Bell':'Philadelphia','Rockwall':'Dallas','Bridgeville':'Pittsburgh',\n",
    "    'North Hollywood':'Los Angeles','Belmont':'San Francisco','Richmond':'San Francisco','Greenwood Village':'Denver',\n",
    "    'Gwynedd':'Philadelphia','Plantation':'Miami','Middletown':'New York','Grand Prairie':'Dallas','Union Beach':'New York',\n",
    "    'Warminster':'Philadelphia','Playa del Rey':'Los Angeles','Centennial':'Denver','Snohomish':'Seattle','Fulton':'Washington',\n",
    "    'Adelphi':'Washington','Cork':'Phoenix','Mesa':'Phoenix','Bala-Cynwyd':'Philadelphia','Garland':'Dallas',\n",
    "    'Township of Cranberry':'Pittsburgh','Aliquippa':'Pittsburgh','Hercules':'San Francisco','Harvey':'Chicago',\n",
    "    'Cambridge':'Washington','Carnegie Hill':'New York','Englewood':'Chicago','Tysons':'Washington','San Leandro':'San Francisco',\n",
    "    'Tinley Park':'Chicago','Kirkland':'Seattle','Cranford':'New York','Elmwood Park':'Chicago','San Rafael':'San Francisco'\n",
    "    ,'Northbrook':'Chicago','Orangeburg':'New York','Chandler':'Phoenix','Franklin Lakes':'New York','Wheaton':'Chicago',\n",
    "    'North Wales':'Philadelphia','Radnor':'Philadelphia'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def city_to_region(i):\n",
    "    return regionalize.get(i, i).replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "indeed_data['region']= indeed_data['location'].apply(city_to_region)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87500.0\n"
     ]
    }
   ],
   "source": [
    "median = np.median(indeed_data['salary_transformed'])\n",
    "print median\n",
    "\n",
    "def is_high(x):\n",
    "    if x > median:\n",
    "        return 1\n",
    "    if x <= median:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "indeed_data['is_high'] = indeed_data.salary_transformed.apply(is_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauge (Pre) Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to find out which words in the decription are associated with a high salary so I can account for those in my model/ To do that I can use a count vectorizer for the description column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Count the number of words in each description. I tried using binary and not using binary and the results\n",
    "### were the same for my models, so I used binary to keep things simple.\n",
    "cvec = CountVectorizer(stop_words='english', min_df=5, binary=True ,decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### fitting my data to create the vocabulary\n",
    "cvec_vocab = cvec.fit(indeed_data['desciprtion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### making my matrix dense\n",
    "transformed_desc = cvec_vocab.transform(indeed_data['desciprtion']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### creating a new dataframe with the word counts\n",
    "trans_desc_df = pd.DataFrame(transformed_desc, columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have lots and lots of words, I'll use a random forest model to determine which words are the most important in determining whether a salary is low or high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_f = RandomForestClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_fit = random_f.fit(trans_desc_df, indeed_data['is_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_features = pd.DataFrame(rf_fit.feature_importances_,\n",
    "                                   index = trans_desc_df.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>0.019817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>0.017754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.014634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.011268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.010826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>0.010030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join</th>\n",
       "      <td>0.009949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>0.009664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadoop</th>\n",
       "      <td>0.009021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>0.008954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>0.008594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>0.008437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professional</th>\n",
       "      <td>0.007634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>0.007401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closely</th>\n",
       "      <td>0.006971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sas</th>\n",
       "      <td>0.006602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientists</th>\n",
       "      <td>0.006480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>0.006075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.005535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>0.005530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              importance\n",
       "team            0.019817\n",
       "looking         0.017754\n",
       "scientist       0.014634\n",
       "learning        0.011268\n",
       "science         0.010826\n",
       "leading         0.010030\n",
       "join            0.009949\n",
       "senior          0.009664\n",
       "hadoop          0.009021\n",
       "engineer        0.008954\n",
       "entry           0.008594\n",
       "machine         0.008437\n",
       "professional    0.007634\n",
       "quality         0.007401\n",
       "closely         0.006971\n",
       "sas             0.006602\n",
       "scientists      0.006480\n",
       "research        0.006075\n",
       "lead            0.005535\n",
       "big             0.005530"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_features.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will extract the top 20 most 'predictive' words to include in my final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_words = ['team','looking','scientist','join','senior','learning','science','leading','entry', 'engineer', 'hadoop',\n",
    "               'professional','quality','machine','scientists', 'closely','big','sas','research','lead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to concat those 20 binary columns to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_data= pd.concat([indeed_data, trans_desc_df[useful_words]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating with job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec2 = CountVectorizer(stop_words='english', min_df=5, binary=True,decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvec2_vocab = cvec2.fit(indeed_data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed_title = cvec2_vocab.transform(indeed_data['desciprtion']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_title_df = pd.DataFrame(transformed_title, columns = [x +'_title' for x in cvec2.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3424, 456)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_title_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_f2 = RandomForestClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_fit2 = random_f2.fit(trans_title_df, indeed_data['is_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf2_features = pd.DataFrame(rf_fit2.feature_importances_,\n",
    "                                   index = trans_title_df.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>team_title</th>\n",
       "      <td>0.044183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist_title</th>\n",
       "      <td>0.028470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science_title</th>\n",
       "      <td>0.022838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior_title</th>\n",
       "      <td>0.017138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research_title</th>\n",
       "      <td>0.016721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer_title</th>\n",
       "      <td>0.016513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_title</th>\n",
       "      <td>0.016352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management_title</th>\n",
       "      <td>0.015935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry_title</th>\n",
       "      <td>0.015924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_title</th>\n",
       "      <td>0.015671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis_title</th>\n",
       "      <td>0.014932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadoop_title</th>\n",
       "      <td>0.014884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading_title</th>\n",
       "      <td>0.014644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistical_title</th>\n",
       "      <td>0.014462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_title</th>\n",
       "      <td>0.013820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance\n",
       "team_title           0.044183\n",
       "scientist_title      0.028470\n",
       "science_title        0.022838\n",
       "senior_title         0.017138\n",
       "research_title       0.016721\n",
       "engineer_title       0.016513\n",
       "quality_title        0.016352\n",
       "management_title     0.015935\n",
       "entry_title          0.015924\n",
       "learning_title       0.015671\n",
       "analysis_title       0.014932\n",
       "hadoop_title         0.014884\n",
       "leading_title        0.014644\n",
       "statistical_title    0.014462\n",
       "data_title           0.013820"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2_features.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these are the same as the description column, so I'll pull out the ones that are different: quality, management, research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_titles = ['team_title','scientist_title','science_title','research_title','engineer_title','quality_title',\n",
    "                'management_title','entry_title','learning_title', 'tools_title','statistical_title','hadoop_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_data= pd.concat([final_data, trans_title_df[useful_titles]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dummies for regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the region variable earlier, so now I need to create some dummy variables so I can include them in my model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indeed_locations = pd.get_dummies(final_data['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data = pd.concat([final_data, indeed_locations], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3424, 56)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of my preprocess data is now included in my final_data dataframe.  Now I can plug it into my random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "source": [
    "Since there's only two classifications determined by the median, the baseline accuracy would be 50% - the value we'd get if we assigned it by coin flip - since, by definition, half of the data is below the median and half is above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a Random Forest model to predict High/Low salary using Sklearn. Start by ONLY using the location as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities_cols =[ 'Atlanta',             'Austin',            'Chicago',\n",
    "                   'Dallas',             'Denver',            'Houston',\n",
    "              'Los_Angeles',              'Miami',           'New_York',\n",
    "             'Philadelphia',            'Phoenix',         'Pittsburgh',\n",
    "                 'Portland',      'San_Francisco',            'Seattle',\n",
    "               'Washington']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "### Setting my X and y to just the location features\n",
    "\n",
    "X_cities = final_data[cities_cols]\n",
    "y = final_data['is_high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### instantiating my random forest classifier\n",
    "RF = RandomForestClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Fitting my classifier with my data\n",
    "fit_location = RF.fit(X_cities,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651279293123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "### Doing a cross validation\n",
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "s = cross_val_score(RF, X_cities, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "print np.mean(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effectivenes of the model just using location is about 65%.  Not bad, and definitely better than the 50% minimum effectives.  Clearly location has some impact on whether a job is above or below the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "068dc1cf-7fd7-4f27-a1f1-7f0a5a221d29"
   },
   "source": [
    "## Including more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a job title.\n",
    "- For example, create a feature that represents whether 'Senior' is in the title \n",
    "- or whether 'Manager' is in the title. \n",
    "- Then build a new Random Forest with these features. Do they add any value? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Adding all the columns I want to include in my primary random forest model\n",
    "features_for_rt = cities_cols + useful_titles + useful_words \n",
    "# features_for_rt =  useful_titles + useful_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### setting my X values to include all of the columns I want\n",
    "X = final_data[features_for_rt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creating a test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Instantiating a new random forest classifier\n",
    "RF =RandomForestClassifier(n_estimators=800, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### fitting my model on my train data\n",
    "fit_all_rf = RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869453195031\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "s = cross_val_score(RF, X, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "print np.mean(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85700389105058361"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86 percent is much better! What features were the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_all_features = pd.DataFrame(fit_all_rf.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>0.049939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientists</th>\n",
       "      <td>0.046488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>0.037903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team_title</th>\n",
       "      <td>0.036153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools_title</th>\n",
       "      <td>0.034082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistical_title</th>\n",
       "      <td>0.033483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>0.031689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle</th>\n",
       "      <td>0.030474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professional</th>\n",
       "      <td>0.027199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New_York</th>\n",
       "      <td>0.026899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance\n",
       "looking              0.049939\n",
       "scientists           0.046488\n",
       "team                 0.037903\n",
       "team_title           0.036153\n",
       "tools_title          0.034082\n",
       "statistical_title    0.033483\n",
       "Washington           0.031689\n",
       "Seattle              0.030474\n",
       "professional         0.027199\n",
       "New_York             0.026899"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_all_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              \"max_depth\": [None, 1, 3, 5,7],\n",
    "              \"max_features\": [.1,1,5, None, 'auto',],\n",
    "              \"min_samples_split\": [ .1, 2, 3,5],\n",
    "              \"min_samples_leaf\": [.1, 1,2,3,5],\n",
    "              \"bootstrap\": [True, False],\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-405-e274898384a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n\u001b[1;32m      7\u001b[0m       % (time() - start, len(grid_search.cv_results_['params'])))\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    581\u001b[0m             delayed(parallel_helper)(e, 'predict_proba', X,\n\u001b[1;32m    582\u001b[0m                                       check_input=False)\n\u001b[0;32m--> 583\u001b[0;31m             for e in self.estimators_)\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time \n",
    "\n",
    "grid_search = GridSearchCV(RF, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "g_boost = GradientBoostingClassifier(n_estimators=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_boost_model= g_boost.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852212404491\n"
     ]
    }
   ],
   "source": [
    "print np.mean(cross_val_score(g_boost_model, X, y, cv=cv, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is not any better than my random forest model, I'll just ignore these results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model - Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second model I'll use Logistic Regression to determine the probability that each salary is above or below the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "from  sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_model = logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Val Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773359884319\n",
      "0.784223706177\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(log_model, X_train, y_train, cv=5)\n",
    "print np.mean(scores)\n",
    "print log_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77431906614785995"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the feature coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.40455725,  0.21954755,  0.61367884, -0.96346171, -0.65505709,\n",
       "         0.02236994, -0.44097071, -1.95087525,  0.27962167,  0.8883876 ,\n",
       "        -0.42452388, -0.91332899,  0.31531901,  0.62779012, -0.72586448,\n",
       "         0.58905321,  0.69364489,  0.49030979,  0.71371636, -0.44977116,\n",
       "         1.52034543, -0.69258399, -0.85867116, -1.54831926,  1.03008709,\n",
       "         1.01332388, -0.19846265,  1.5086188 ,  0.69364489,  1.18079518,\n",
       "         0.49030979,  0.96129563,  1.24778926,  1.03008709,  0.71371636,\n",
       "         2.62133321, -1.54831926,  1.52034543,  1.5086188 ,  3.11935861,\n",
       "        -0.69258399, -0.40355889,  0.42272854,  2.56970944,  0.48207639,\n",
       "         2.29762364, -0.44977116,  0.6664426 ]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>coef_transformed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>professional</th>\n",
       "      <td>3.119359</td>\n",
       "      <td>22.631859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>2.621333</td>\n",
       "      <td>13.754048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closely</th>\n",
       "      <td>2.569709</td>\n",
       "      <td>13.062029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sas</th>\n",
       "      <td>2.297624</td>\n",
       "      <td>9.950508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer_title</th>\n",
       "      <td>1.520345</td>\n",
       "      <td>4.573805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>1.520345</td>\n",
       "      <td>4.573805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadoop</th>\n",
       "      <td>1.508619</td>\n",
       "      <td>4.520483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadoop_title</th>\n",
       "      <td>1.508619</td>\n",
       "      <td>4.520483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlanta</th>\n",
       "      <td>1.404557</td>\n",
       "      <td>4.073723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>1.247789</td>\n",
       "      <td>3.482635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>1.180795</td>\n",
       "      <td>3.256963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>1.030087</td>\n",
       "      <td>2.801310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_title</th>\n",
       "      <td>1.030087</td>\n",
       "      <td>2.801310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools_title</th>\n",
       "      <td>1.013324</td>\n",
       "      <td>2.754742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join</th>\n",
       "      <td>0.961296</td>\n",
       "      <td>2.615082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia</th>\n",
       "      <td>0.888388</td>\n",
       "      <td>2.431206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.713716</td>\n",
       "      <td>2.041564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science_title</th>\n",
       "      <td>0.713716</td>\n",
       "      <td>2.041564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team_title</th>\n",
       "      <td>0.693645</td>\n",
       "      <td>2.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>0.693645</td>\n",
       "      <td>2.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.666443</td>\n",
       "      <td>1.947298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San_Francisco</th>\n",
       "      <td>0.627790</td>\n",
       "      <td>1.873466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago</th>\n",
       "      <td>0.613679</td>\n",
       "      <td>1.847215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>0.589053</td>\n",
       "      <td>1.802281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.490310</td>\n",
       "      <td>1.632822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist_title</th>\n",
       "      <td>0.490310</td>\n",
       "      <td>1.632822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>0.482076</td>\n",
       "      <td>1.619433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientists</th>\n",
       "      <td>0.422729</td>\n",
       "      <td>1.526120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland</th>\n",
       "      <td>0.315319</td>\n",
       "      <td>1.370697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New_York</th>\n",
       "      <td>0.279622</td>\n",
       "      <td>1.322629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austin</th>\n",
       "      <td>0.219548</td>\n",
       "      <td>1.245513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Houston</th>\n",
       "      <td>0.022370</td>\n",
       "      <td>1.022622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistical_title</th>\n",
       "      <td>-0.198463</td>\n",
       "      <td>0.819990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>-0.403559</td>\n",
       "      <td>0.667939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>-0.424524</td>\n",
       "      <td>0.654081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Los_Angeles</th>\n",
       "      <td>-0.440971</td>\n",
       "      <td>0.643412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>-0.449771</td>\n",
       "      <td>0.637774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research_title</th>\n",
       "      <td>-0.449771</td>\n",
       "      <td>0.637774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denver</th>\n",
       "      <td>-0.655057</td>\n",
       "      <td>0.519412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_title</th>\n",
       "      <td>-0.692584</td>\n",
       "      <td>0.500282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.692584</td>\n",
       "      <td>0.500282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seattle</th>\n",
       "      <td>-0.725864</td>\n",
       "      <td>0.483906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management_title</th>\n",
       "      <td>-0.858671</td>\n",
       "      <td>0.423725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pittsburgh</th>\n",
       "      <td>-0.913329</td>\n",
       "      <td>0.401186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dallas</th>\n",
       "      <td>-0.963462</td>\n",
       "      <td>0.381570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>-1.548319</td>\n",
       "      <td>0.212605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry_title</th>\n",
       "      <td>-1.548319</td>\n",
       "      <td>0.212605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miami</th>\n",
       "      <td>-1.950875</td>\n",
       "      <td>0.142150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       coef  coef_transformed\n",
       "professional       3.119359         22.631859\n",
       "leading            2.621333         13.754048\n",
       "closely            2.569709         13.062029\n",
       "sas                2.297624          9.950508\n",
       "engineer_title     1.520345          4.573805\n",
       "engineer           1.520345          4.573805\n",
       "hadoop             1.508619          4.520483\n",
       "hadoop_title       1.508619          4.520483\n",
       "Atlanta            1.404557          4.073723\n",
       "senior             1.247789          3.482635\n",
       "looking            1.180795          3.256963\n",
       "learning           1.030087          2.801310\n",
       "learning_title     1.030087          2.801310\n",
       "tools_title        1.013324          2.754742\n",
       "join               0.961296          2.615082\n",
       "Philadelphia       0.888388          2.431206\n",
       "science            0.713716          2.041564\n",
       "science_title      0.713716          2.041564\n",
       "team_title         0.693645          2.000996\n",
       "team               0.693645          2.000996\n",
       "lead               0.666443          1.947298\n",
       "San_Francisco      0.627790          1.873466\n",
       "Chicago            0.613679          1.847215\n",
       "Washington         0.589053          1.802281\n",
       "scientist          0.490310          1.632822\n",
       "scientist_title    0.490310          1.632822\n",
       "big                0.482076          1.619433\n",
       "scientists         0.422729          1.526120\n",
       "Portland           0.315319          1.370697\n",
       "New_York           0.279622          1.322629\n",
       "Austin             0.219548          1.245513\n",
       "Houston            0.022370          1.022622\n",
       "statistical_title -0.198463          0.819990\n",
       "machine           -0.403559          0.667939\n",
       "Phoenix           -0.424524          0.654081\n",
       "Los_Angeles       -0.440971          0.643412\n",
       "research          -0.449771          0.637774\n",
       "research_title    -0.449771          0.637774\n",
       "Denver            -0.655057          0.519412\n",
       "quality_title     -0.692584          0.500282\n",
       "quality           -0.692584          0.500282\n",
       "Seattle           -0.725864          0.483906\n",
       "management_title  -0.858671          0.423725\n",
       "Pittsburgh        -0.913329          0.401186\n",
       "Dallas            -0.963462          0.381570\n",
       "entry             -1.548319          0.212605\n",
       "entry_title       -1.548319          0.212605\n",
       "Miami             -1.950875          0.142150"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = pd.DataFrame(log_model.coef_, columns = features_for_rt)\n",
    "logs = logs.transpose()\n",
    "logs.columns = ['coef']\n",
    "\n",
    "logs['coef_transformed'] = np.exp(logs.coef)\n",
    "\n",
    "logs.sort('coef_transformed', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_fit = knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print knn.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(cross_val_score(knn, X, y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_list= [              'team',\n",
    "                  'looking',          'scientist',               'join',\n",
    "                   u'senior',           u'learning',            u'science',\n",
    "                  u'leading',              u'entry',           u'engineer',\n",
    "                   u'hadoop',       u'professional',            u'quality',\n",
    "                  u'machine',         u'scientists',         u'team_title',\n",
    "          u'scientist_title',      u'science_title',     u'research_title',\n",
    "           u'engineer_title',      u'quality_title',   u'management_title',\n",
    "              u'entry_title',     u'learning_title',        u'tools_title',\n",
    "        u'statistical_title',       u'hadoop_title',            u'Atlanta',\n",
    "                   u'Austin',            u'Chicago',             u'Dallas',\n",
    "                   u'Denver',            u'Houston',        u'Los_Angeles',\n",
    "                    u'Miami',           u'New_York',       u'Philadelphia',\n",
    "                  u'Phoenix',         u'Pittsburgh',           u'Portland',\n",
    "            u'San_Francisco',            u'Seattle',         u'Washington']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3424"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_data['entry_title'])\n",
    "len(final_data['entry_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratios_df = pd.DataFrame(columns = ['feature','total_high','total','ratio'])\n",
    "\n",
    "for col in col_list:\n",
    "    d = sum(final_data[col][final_data[col]==1])\n",
    "    r = sum(final_data[col][final_data['is_high']==1])\n",
    "#     r = (len(final_data[col]))\n",
    "    try:\n",
    "        ratio = float(r) / d\n",
    "    except:\n",
    "        ratio = None\n",
    "    lit = [{'feature':col, 'total_true':d , 'total_high': r , 'ratio':ratio }]\n",
    "    ratios_df = ratios_df.append(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>ratio</th>\n",
       "      <th>total</th>\n",
       "      <th>total_high</th>\n",
       "      <th>total_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>professional</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>engineer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>engineer_title</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leading</td>\n",
       "      <td>0.981651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hadoop_title</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hadoop</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>join</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning_title</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>senior</td>\n",
       "      <td>0.896739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>0.890710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163.0</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>looking</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.0</td>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>0.817460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science_title</td>\n",
       "      <td>0.817460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>team_title</td>\n",
       "      <td>0.792419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>439.0</td>\n",
       "      <td>554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>team</td>\n",
       "      <td>0.792419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>439.0</td>\n",
       "      <td>554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>0.692521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.0</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San_Francisco</td>\n",
       "      <td>0.683805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>266.0</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tools_title</td>\n",
       "      <td>0.683761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientist_title</td>\n",
       "      <td>0.665615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633.0</td>\n",
       "      <td>951.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientist</td>\n",
       "      <td>0.665615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633.0</td>\n",
       "      <td>951.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>0.657754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.0</td>\n",
       "      <td>187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington</td>\n",
       "      <td>0.633907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.0</td>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientists</td>\n",
       "      <td>0.623907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>428.0</td>\n",
       "      <td>686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New_York</td>\n",
       "      <td>0.529820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>382.0</td>\n",
       "      <td>721.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statistical_title</td>\n",
       "      <td>0.430556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Portland</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Los_Angeles</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austin</td>\n",
       "      <td>0.351515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Houston</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research_title</td>\n",
       "      <td>0.320366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140.0</td>\n",
       "      <td>437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>management_title</td>\n",
       "      <td>0.308411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>0.206790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "      <td>324.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denver</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quality</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quality_title</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miami</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entry</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entry_title</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature     ratio total  total_high  total_true\n",
       "0       professional  1.000000   NaN        53.0        53.0\n",
       "0           engineer  1.000000   NaN        92.0        92.0\n",
       "0     engineer_title  1.000000   NaN        92.0        92.0\n",
       "0            leading  0.981651   NaN       107.0       109.0\n",
       "0       hadoop_title  0.960000   NaN        96.0       100.0\n",
       "0             hadoop  0.960000   NaN        96.0       100.0\n",
       "0               join  0.935673   NaN       160.0       171.0\n",
       "0           learning  0.902439   NaN       185.0       205.0\n",
       "0     learning_title  0.902439   NaN       185.0       205.0\n",
       "0             senior  0.896739   NaN       165.0       184.0\n",
       "0            machine  0.890710   NaN       163.0       183.0\n",
       "0            looking  0.860215   NaN       320.0       372.0\n",
       "0            science  0.817460   NaN       206.0       252.0\n",
       "0      science_title  0.817460   NaN       206.0       252.0\n",
       "0         team_title  0.792419   NaN       439.0       554.0\n",
       "0               team  0.792419   NaN       439.0       554.0\n",
       "0            Chicago  0.692521   NaN       250.0       361.0\n",
       "0      San_Francisco  0.683805   NaN       266.0       389.0\n",
       "0        tools_title  0.683761   NaN        80.0       117.0\n",
       "0    scientist_title  0.665615   NaN       633.0       951.0\n",
       "0          scientist  0.665615   NaN       633.0       951.0\n",
       "0       Philadelphia  0.657754   NaN       123.0       187.0\n",
       "0            Atlanta  0.653061   NaN        64.0        98.0\n",
       "0         Washington  0.633907   NaN       258.0       407.0\n",
       "0         scientists  0.623907   NaN       428.0       686.0\n",
       "0           New_York  0.529820   NaN       382.0       721.0\n",
       "0             Dallas  0.471429   NaN        33.0        70.0\n",
       "0  statistical_title  0.430556   NaN        93.0       216.0\n",
       "0           Portland  0.424242   NaN        28.0        66.0\n",
       "0        Los_Angeles  0.402985   NaN       135.0       335.0\n",
       "0             Austin  0.351515   NaN        58.0       165.0\n",
       "0            Houston  0.326087   NaN        15.0        46.0\n",
       "0     research_title  0.320366   NaN       140.0       437.0\n",
       "0   management_title  0.308411   NaN        66.0       214.0\n",
       "0            Seattle  0.206790   NaN        67.0       324.0\n",
       "0             Denver  0.190476   NaN        16.0        84.0\n",
       "0         Pittsburgh  0.181818   NaN         4.0        22.0\n",
       "0            quality  0.123457   NaN        20.0       162.0\n",
       "0      quality_title  0.123457   NaN        20.0       162.0\n",
       "0            Phoenix  0.121622   NaN         9.0        74.0\n",
       "0              Miami  0.026667   NaN         2.0        75.0\n",
       "0              entry  0.000000   NaN         0.0       108.0\n",
       "0        entry_title  0.000000   NaN         0.0       108.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios_df.sort_values(by='ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87500.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(final_data['salary_transformed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_features[:20].to_csv('~/desktop/CSV/finaldatasummary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_data.to_csv('~/desktop/CSV/finaldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf2_features.to_csv('~/desktop/CSV/foranalysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_all_features.to_csv('~/desktop/CSV/all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3be94357-e551-4094-b784-2df039216d33"
   },
   "source": [
    "### BONUS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "#### Bonus: Use Count Vectorizer from scikit-learn to create features from the text summaries. \n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
